> Прежде всего речь пойдёт про **партицирование (partition)**.
> Это метод разбиение кучи данных на сегменты.

# Partition types
## Vertical
> Таблицу разбивают на несколько подтаблиц с различными колонками (атрибутами)

![](image-storage/1_1721034397.png)
* Полезно, когда запросам приходится сканировать огромное кол-во атрибутов (столбцов)
## Hotizontal, Sharding 
> Горизонтальным (или же шардирование) - данные одной таблицы распределяются на несколько

![](image-storage/2_1721034413.png)	
* Основной смысл горизонтального масштабирования (т.е. если не хватает 1 "трубки" и её пропускной способности, то можно сделать много "трубок")
* Не единая точка отказа
* Ускоряет запросы (т.к. порой одинаковые запросы могут обращаться к разным шардам, а не к одной и то же таблице/БД)
* Но неравномерная нагрузка серверов
* Сложные запросы могут затрагивать несколько шардов и долго обрабатываться
* Распределённость всегда ведёт к сложному мониторингу, так и с этим случаем...
# Стратегии
![](image-storage/image_90_1740042112.png)
## key-based
> Значения столбцов хэшируются и по хэш значению распределяются

![](image-storage/database_sharding_key_based_7242ac8dea.png)
* Около-равномерное распределение данных, и это **важнейший критерий** тут
* Высокая производительность из-за природы хэша
* Иногда трудно искать некоторую инфу
## range-based
> Основано на диапазоне ключей

![Pasted image 20241027140222](image-storage/Pasted%20image%2020241027140222.png)
* Легко искать
* Легко реализовать самописный алгоритм
* Возможна сильная неравномерность шардов
## dynamic
> Довольно хитрый механизм, который шардирует данные по своим алгоритмам на основе нагрузки на БД
## geo
> Расположение данных зависит от географической зоны
## directory-based
> Работает по принципу DNS, есть центральный шард, который в случае чего, знает куда обратиться для получения нужных данных

![lookup-based-sharding](image-storage/lookup-based-sharding.webp)